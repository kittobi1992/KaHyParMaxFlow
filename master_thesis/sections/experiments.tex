In this Section, we evaluate the performance of our flow-based refinement framework. 
First, we show the effects of our sparsification techniques of the Lawler-Network 
\cite{lawler1973} on the performance of our maximum flow algorithms.
Afterwards, we analyze how the maximum
flow problem size influences the solution quality of different configurations of
our framework. Finally, we compare the new version of \emph{KaHyPar} with
\emph{Max-Flow-Min-Cut} computations against with
state-of-the-art hypergraph partitioners.


\subsection{Instances}

Our full benchmark set consists of $488$ hypergraphs from three different 
application areas. For VLSI design we use instances from
the \emph{ISPD98 VLSI Circuit Benchmark Suite} (\ISPD) \cite{alpert1998ispd98} and add more recent
instances of the \emph{DAC 2012 Routability-Driven Placement Contest} (\DAC) \cite{viswanathan2012dac}.
We interpret the Sparse Matrix instances of the \emph{Florida Sparse Matrix 
Collection} (\SPM) \cite{davis2011university} as hypergraphs using the row-net model \cite{catalyurek1999hypergraph}.
The rows of each matrix are treated as hyperedges and the columns are the vertices of
the hypergraph. Our last benchmark type are SAT formulas of the \emph{International SAT
Competition 2014} \cite{belov2014application}. A common interpretation of a SAT formula 
as hypergraph is to interpret the literals as vertices and each clause as a net (\Literal) \cite{papa2007hypergraph}.
Mann and Papp \cite{mann2014formula} suggested two other hypergraph representation of
SAT formulas, called \Primal~and \Dual. The \Primal~representation treats each variable
as vertex and each clause as hyperedge. The \Dual~representation treats each clause as
vertex and the variables induces nets containing all clauses where the corresponding
variable occurs. A summary of the different instance types is presented in
Table \ref{tbl:full_benchmark_set}. \\
We divide our full benchmark set into two smaller subsets. Our \emph{parameter tuning}
benchmark set consists of $25$ hypergraphs, $5$ of each instance type (except \DAC). Additionally,
we choose a benchmark subset of $165$ instances. On our general experiments we partition
each hypergraph into $k \in \{2,4,8,16,32,64,128\}$ blocks and use $10$ different
\emph{seeds} for each $k$ and an imbalance of $\epsilon = 3\%$.

\subsection{System and Methodology}
\label{sec:methodology}

Our experiments run on a single core of a machine consisting of two \emph{Intel Xeon E5-
2670 Octa-Core} processors clocked at $2.6$ GHz. The machine has $64$ GB main memory,
$20$ MB L$3$- and $8\times256$ KB L$2$-Cache. The code is written in C++ and compiled using
\lstinline{g++-5.2} with flags \lstinline{-O3 -mtune=nactive -march=native}. We refer to
our new implementation of \emph{KaHyPar} with \emph{(M)ax-(F)low-Min-Cut} computations 
as \KaHyPar{MF} and the latest configuration with \emph{(C)ommunity-(A)ware} coarsening as
\KaHyPar{CA}. \\
We compare \KaHyPar{MF} with the state-of-the-art hypergraph partitioners \emph{hMetis} 
\cite{karypis1999multilevel,karypis2000multilevel} and \emph{PaToH} \cite{catalyurek1999hypergraph}.
\emph{hMetis} provides a direct $k$-way (\hMetis{K}) and recursive bisection (\hMetis{R}) implementation.
Further, we use the default configuration (\PaToH{D}) and quality preset (\PaToH{Q}) of
\emph{PaToH}. We configure \emph{hMetis} to optimize the \emph{sum-of-external-degree}-metric
(SOED) and calculate $(\lambda-1)(\Pi) = \text{SOED}(\Pi) - \text{cut}(\Pi)$. This is also
suggested by the authors of \emph{hMetis} \cite{karypis2000multilevel}. Additionally, we have
to adapt the imbalance definition of \hMetis{R}. An imbalance value of $5$ means that the weight
of each bisected block is allowed to be between $0.45 \cdot c(V)$ and $0.55 \cdot c(V)$.
To ensure that \hMetis{R} produces a valid $\epsilon$-balanced partition after $\log_2(k)$
bisections we have to adapt $\epsilon$ to
\[\epsilon' = 100 \cdot \Bigg( \left( (1 + \epsilon) \frac{\lceil \frac{c(V)}{k} \rceil}{c(V)} \right)^{\frac{1}{\log_2(k)}} - 0.5 \Bigg)\]
If we evaluate the performance of our hypergraph partitioner,
we first summarize the results by calculating the arithmetic mean (or minimum) of a specific metric
for the diffrent \emph{seeds} of a hypergraph instance partitioned into $k$ blocks. 
Afterwards, we calculate the \emph{geometric mean} of 
all instances to give each instance comparable influence on the final result. 
To compare the performance of different hypergraph partitioners in more detail
we use performance plots introduced in \cite{schlag2016k}.
In the following, we will call a $k$-way partition of a hypergraph an \emph{instance}.
For each algorithm we determine the instance with the 
minimum cut (of the $10$ different \emph{seeds}). Each color in the plot corresponds to
one algorithm. The $x$-axis represents the number of instances and the $y$-axis represents the quality ratio
produced by an algorithm for an instance relative to the partition of the best algorithm. 
For example, partitioner $P_1$ produces a partition for an instance $X$ with
quality $100$ and partitioner $P_2$ produces a partition for the same instance of quality $105$.
Then, the $y$-value for instance $X$ of partitioner $P_2$ is $1 - \frac{105}{100} = 0.05$ and
for partitioner $P_1$ is $1 - \frac{100}{100} = 0$, which means that the partition of partitioner $P_2$
for instance $X$ is $5\%$ worse than the best partition produced for instance $X$.
 A value of zero indicates that the algorithm
produced the best partition. A point close to one indicates that the partition produced by the 
corresponding partitioner was considerably worse than the partition produced by the best
algorithm. Before we insert the points in the grid, we sort them in decreasing order 
according to the $y$-values. An algorithm is considered to outperform another algorithm if its 
corresponding ratio values are below those of the other algorithm. A point with an $y$-value
greater than one corresponds to an infeasible solution that violated the balanced constraint.

\subsection{Flow Algorithms and Networks}
\label{sec:exp_flow_network}

In the first experiment, we evaluate the effect of our sparsification techniques
on the performance of our maximum flow algorithms \EdmondKarp, \GoldbergTarjan, \BoykovKolmogorov~and \IBFS. 
We refer to the Lawler-Network as $\ExpLawler$, which is our baseline flow network. In
Section \ref{sec:opt_flow_network} we present several techniques to reduce the number
of nodes and edges of $\ExpLawler$. $\ExpNodeDegree$ represents our flow network in which
we remove all hypernodes with a degree smaller or equal to $3$. The network $\ExpEdgeSize$ models
each hyperedge of size $2$ as undirected graph edge between the corresponding pins. Finally,
$\ExpHybrid$ combines both networks. \\
We evaluate the performance of our maximum flow algorithms on flow problems with
$|V'| \in \{500,1000,5000,10000,25000\}$ hypernodes. The instances are generated by executing
\emph{KaHyPar} on our benchmark subset (see Table \ref{tbl:benchmark_subset}) for $k = 2$ and five different 
seeds. After an instance is bipartitioned, we generate flow problems
with the above-mentioned sizes and execute each possible combination of flow algorithm and
flow network on it. \\
\autoref{fig:node_edge_distribution}~shows the number of nodes and edges
of the resulting flow networks for flow problems with $25000$ hypernodes. As expected,
$\ExpNodeDegree$ reduces the number of nodes more significantly on low hypernode degree
instances (\Dual) and $\ExpEdgeSize$ more on small hyperedge size instances
(\Primal~and \Literal). Further, $\ExpHybrid$ combines the advantages of both networks
and reduces the number of nodes and edges of nearly each benchmark type by at least a factor of $2$,
except on \SPM~instances. If we compare the sizes of the resulting flow problem, we can observe
that instances with a high density ($d = \frac{|E|}{|V|}$), like \Primal~or \Literal, yield large flow problem 
instances and instances with a low density yield small flow problem
instances (see \Dual~instances). \\
\begin{figure}[ht!]
\centering
\input{experiments/flow_network/node_edge_distribution.tex} %
\caption{Comparison of the number of nodes and edges induced by flow problems 
         of size $|V'| = 25000$ on our flow network for different benchmark types.
         The red dashed lines indicates $25000$ nodes.}
\label{fig:node_edge_distribution}
\end{figure} 
In \autoref{fig:max_flow_network_algo}~we compare the performance of our maximum flow algorithms
on different flow networks. A bar in the plot indicates the speed-up of the corresponding algorithm
executed on $\ExpNodeDegree$, $\ExpEdgeSize$ or $\ExpHybrid$ relative to the execution on $\ExpLawler$.
The main observation is that the speed-ups are nearly proportional to the reduction of the number
of nodes and edges of the corresponding flow network. For example, $\ExpHybrid$ reduces the size
of the flow problems of nearly each benchmark type by at least factor of $2$ compared to $\ExpLawler$. Consequently,
the speed-ups of our maximum flow algorithms on $\ExpHybrid$ are approximately to $2$ on each
benchmark type. Our \GoldbergTarjan~implementation profits most from our sparsification 
techniques. The algorithm is around $3$ to $4$ times faster on $\ExpHybrid$ than on 
$\ExpLawler$. Furhter, all algorithms perform best on $\ExpHybrid$. In conclusion, our
sparsification techniques not only reduces the size of the flow problems, it also significantly
speed-ups the running time of several maximum flow algorithms.\\
\begin{figure}
\centering
\input{experiments/flow_network/speed_up_flow_network.tex} %
\caption{Speed-ups of our flow algorithms on different flow networks relative to execution on
         $\ExpLawler$ for different problem sizes and types. The red resp. blue dashed line 
         indicates a speed-up of $1$ resp. $2$.}
\label{fig:max_flow_network_algo}
\end{figure} 
In \autoref{tbl:flow_algo_network_summary}, we compare the absolute running times of the 
algorithms on our fastest flow network $\ExpHybrid$. The \IBFS~algorithm works best on
large instances ($|V'| > 1000$). For smaller benchmarks ($|V'| \le 1000$) algorithms
based on the \emph{augmenting path} concept, like \BoykovKolmogorov~or \EdmondKarp, are faster
than \IBFS~and \GoldbergTarjan. However, we are currently not able to use the \IBFS~algorithm
in our \emph{flow}-based local search algorithm. The data structure of the algorithm
is not optimized for multiple executions on different flow networks, because it did not 
implement an efficient strategy to reuse the allocated memory. The usage of the \IBFS~algorithm in our
framework leads to memory overflows. However, we currently work on a reimplementation
of the algorithm such that it can be used. Therefore, we choose the \BoykovKolmogorov~maximum 
flow algorithm in combination with our flow network $\ExpHybrid$ in the following experiments.
\begin{table}
\renewcommand{\arraystretch}{1.15}
\footnotesize
\centering
\begin{tabular}{r|C{2.5cm}|c|c|c}
\toprule
\quad\quad & \IBFS & \BoykovKolmogorov & \GoldbergTarjan & \EdmondKarp \\
$|V'|$ &  $t[ms]$ & $t[\%]$ & $t[\%]$ & $t[\%]$ 
\\\midrule% 
\csname @@input\endcsname experiments/flow_network/flow_network_max_flow_summary_table.tex 
\bottomrule
\end{tabular}
\caption{Absolute running time of our maximum flow algorithms on flow network $\ExpHybrid$.
         Note, all values in the table are in percentage relative to the running time
         of the \IBFS algorithm. In each line the fastest variant is marked bold.}
\label{tbl:flow_algo_network_summary}
\end{table}

\subsection{Setup of the direct $k$-way Flow-Based Refinement}
\label{sec:flow_configuration}

In this Section, we analyze the quality of our $k$-way flow-based refinement algorithm with
different configurations on our parameter tuning benchmark subset (see Table \ref{tbl:parameter_tunning_set}).
There are several configurations and tuning parameters which we have to evaluate:
\begin{itemize}
\item \emph{Max-(F)low-Min-Cut} computations as refinement algorithm (see Section \ref{sec:flow_local_search_hypergraph})
\item \emph{Adaptive Flow Iteration} parameter $\alpha'$ (see Section \ref{sec:adaptive_flow_iterations})
\item \emph{(M)ost Balanced Minimum Cut} heuristic (see Section \ref{sec:mbmc_hypergraphs})
\item Combining \emph{Max-(F)low-Min-Cut} computations with \emph{(FM)} refinement
\end{itemize}
In the following, we will denote a configuration e.g. with \FlowVariant{+}{-}{-} which indicates
which heuristic resp. technique is enabled $(+)$ or disabled $(-)$. The meaning of the 
abbreviations is explained in the enumeration above (see letters inside parentheses). We evaluate
a configuration for $k \in \{2,4,8,16,32,64,128\}$, $\alpha' \in \{1,2,4,8,16\}$
and $10$ different seeds on our parameter tuning benchmark subset ($\epsilon = 3\%$). 
We execute a \emph{flow}-based \emph{local search} 
on each level $i$ with $i = 2^j$ ($j \in \mathbb{N}_+$) 
(see Section \ref{sec:flow_local_search_hypergraph}). Additionally, we add configuration
\FlowVariant{+}{+}{+} with \emph{flow execution policy} $i = 128j$ ($j \in \mathbb{N}_+$). 
This configuration has an impracticable running time, but should provide a lower bound
of the solution quality achievable with \emph{Max-Flow-Min-Cut} computations in combination
with \emph{FM} refinement.
We refer to this variant as \Constant{128}. Further, we use the 
latest version of \emph{KaHyPar} as reference \cite{heuer2017improving}. We refer to this variant as 
\FlowVariant{-}{-}{+}. \\
The results are summarized in \autoref{tbl:alpha_exp}. The values
in the column \emph{Avg} are improvements of the connectivity metric
relative to our baseline configuration \FlowVariant{-}{-}{+}. The running
time are absolute values in seconds. The first observation is that flows on
its own as refinement strategy are not strong enough to outperform the
\emph{FM} heuristic. Our strongest configuration with $\alpha' = 16$
is $2.58\%$ worse than our \emph{FM} baseline. But the result 
is still remarkable because we only execute flows on $\log{n}$ levels
instead of $n$ as the \emph{FM} algorithm is executed. The running time
scales nearly linear with parameter $\alpha'$. \\
Enabling the \emph{Most Balanced Minimum Cut} heuristic significantly improves the
quality compared to the configuration \FlowVariant{+}{-}{-}.
But the quality improvements are more significant for large
$\alpha'$. The larger the flow problem size, the larger is the number of different minimum 
$(S,T)$-cutsets and this increases the possibility to find a feasible solution according to 
our balanced constraint. Also it outperforms our baseline \emph{FM} 
configuration for $\alpha' = 16$ by $0.51\%$.\\
If we enable \emph{FM} refinement in all levels where no flow is executed, we improve the solution
quality by nearly $2\%$ (for $\alpha' = 16$). Also, the running time of this variant is faster
than all previous flow configurations because we transfer more work to the \emph{FM} refinement.
It has as a consequence that a block becomes faster \emph{inactive} during the \emph{active block 
scheduling} algorithm and this decreases the number of rounds of complete pairwise 
flow-based refinements. \\
Finally, \Constant{128} gives us a lower bound of the quality
achievable with a combination of \emph{flow}-based and \emph{FM} refinement. Flows are executed 
in each $128$th level of the multilevel hierarchy. The quality is $2.44\%$ better than our
baseline configuration, but $\approx 100$ times slower. Compared to \FlowVariant{+}{+}{+} for 
$\alpha = 16$, \Constant{128} is only $0.57\%$ better and around $\approx 25$ times slower.\\
Our best configuration is \FlowVariant{+}{+}{+} with $\alpha' = 16$. It is also the most 
effective one (see Effectiveness Test in Appendix \ref{appendix:effectiveness_test}). For
further experiments, we refer to this variant as \KaHyPar{MF}.

\begin{table}
\renewcommand{\arraystretch}{1.15}
\centering
\begin{tabular}{|r||c|c||c|c||c|c|c|c|}
\toprule
 Config. & \multicolumn{2}{c||}{\FlowVariant{+}{-}{-}} & \multicolumn{2}{c||}{\FlowVariant{+}{+}{-}}  & \multicolumn{2}{c|}{\FlowVariant{+}{+}{+}} & \multicolumn{2}{c|}{\Constant{128}} \\
\midrule
$\alpha'$ & Avg.$[\%]$ & $t[s]$ & Avg.$[\%]$ & $t[s]$ & Avg.$[\%]$ & $t[s]$ & Avg.$[\%]$ & $t[s]$ \\
\midrule%
\csname @@input\endcsname experiments/flow_alpha/flow_alpha_table.tex 
\bottomrule
 Config. & \multicolumn{2}{c||}{\FlowVariant{+}{-}{-}} & \multicolumn{2}{c||}{\FlowVariant{+}{+}{-}}  & \multicolumn{2}{c|}{\FlowVariant{+}{+}{+}} & \multicolumn{2}{c|}{\Constant{128}} \\
\midrule
$\alpha'$ & Avg.$[\%]$ & $t[s]$ & Avg.$[\%]$ & $t[s]$ & Avg.$[\%]$ & $t[s]$ & Avg.$[\%]$ & $t[s]$ \\
\midrule%
\csname @@input\endcsname experiments/flow_alpha/flow_alpha_table_old.tex 
\bottomrule
\end{tabular}
\caption{ Table contains results for different configurations of our flow-based refinement
          framework for increasing $\alpha'$. The quality in column \emph{Avg.} is relative
          to our baseline configuration without the usage of flows. }
\label{tbl:alpha_exp}
\end{table}


\subsection{Speed-Up Heuristics}
\label{sec:speed_up}

At the end of Section \ref{sec:flow_local_search_hypergraph}, we have presented several heuristics
to prevent \emph{unpromising} flow executions during the \emph{active block scheduling} refinement ((R1)-(R3)).
The main assumption is that only a minority of \emph{Max-Flow-Min-Cut} computations
lead to an improvement. To verify this assumption, 
we execute \KaHyPar{MF} in combination with different speed-up heuristics 
on our benchmark subset (see Table \ref{tbl:benchmark_subset}). \\
Table \ref{tbl:heuristics} summarizes the results of the experiment. \KaHyPar{CA} is the current
version of \emph{KaHyPar} and \KaHyPar{MF} is our current flow configuration of
Section \ref{sec:flow_configuration}. The indices of the different variants of \KaHyPar{MF} 
describes which speed-up heuristic is enabled (see Section \ref{sec:flow_local_search_hypergraph}).
On average, enabling all speed-up heuristics worsen the quality of \KaHyPar{MF} only by 
$0.09\%$. On the other hand, the framework is significantly faster 
by a factor of $\approx 2$. In its final configuration \KaHyParConfig{MF}{R1,R2,R3} computes 
partitions with $2\%$ better quality ($(\lambda - 1)$-metric) than \KaHyPar{CA}, while only
incurring a slowdown of a factor of $2$. In the following, we will denote our final 
configuration \KaHyParConfig{MF}{R1,R2,R3} with \KaHyPar{MF}.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.15}
\centering
\begin{tabular}{l|cccc}
\toprule
Variant & Avg.$[\%]$ & Min.$[\%]$ & $t_{\text{flow}}[s]$ & $t[s]$ \\
\midrule%
\csname @@input\endcsname experiments/speed_up_heuristics/heuristic_table.tex 
\bottomrule
Variant & Avg.$[\%]$ & Min.$[\%]$ & $t_{\text{flow}}[s]$ & $t[s]$ \\
\midrule%
\csname @@input\endcsname experiments/speed_up_heuristics/heuristic_table_old.tex 
\bottomrule
\end{tabular} 
\caption{Results of our flow-based refinement framework with different speedup heuristics.}
\label{tbl:heuristics}
\end{table}

\subsection{Comparison with other Hypergraph Partitioner}
\label{sec:final_comparison}

Finally, we compare our new approach \KaHyPar{MF} with different state-of-the-art hypergraph
partitioner on our full benchmark set. We excluded $194$ instances of $3416$ either because 
\PaToH{Q} could not allocate enough memory or other partitioners did not finish in time. The
excluded instances are shown in Table \ref{tbl:excluded}. \\
\autoref{fig:final_flow}~summarizes the results of the experiment. \KaHyPar{MF}
produce on $\approx 70\%$ of all benchmark instances the best partitions. It is followed by
\hMetis{R} ($14\%$), \hMetis{K} ($11\%$), \KaHyPar{CA} ($2.4\%$), 
\PaToH{Q} ($1.9\%$) and \PaToH{D} ($1.4\%$). Comparing \KaHyPar{MF} 
individually with each partitioner, \KaHyPar{MF} produces better partitions than \KaHyPar{CA},
\hMetis{R}, \hMetis{K}, \PaToH{Q}, \PaToH{Q} on $96\%$, $80\%$, $82\%$, $95\%$, $95\%$ of the benchmark instances.
Especially on \emph{VLSI} instances, \KaHyPar{MF} calculates significantly better partitions
than all other hypergraph partitioners (see \DAC~and \ISPD~in \autoref{fig:final_flow}).\\
\begin{figure}
\centering
\input{experiments/final_flow/fullset.tex} %
\caption{Min-Cut performance plots comparing \KaHyPar{MF} with \KaHyPar{CA} and
         other systems. Plots are explained in Section \ref{sec:methodology}.}
\label{fig:final_flow}
\end{figure} 
Table \ref{tbl:running_time} shows the running time of all partitioner for different benchmark
types. The running time of \KaHyPar{MF} is within a factor of $2$ slower than \KaHyPar{CA} and
is comparable to the running time of \hMetis{K}. 
\begin{table}[ht!]
\renewcommand{\arraystretch}{1.15}
\centering
\begin{tabular}{c|ccccccc}
\toprule
\multirow{2}{*}{Partitioner} & \multicolumn{7}{c}{Running Time $t[s]$} \\
\cmidrule{2-8}
 & \ALL & \DAC & \ISPD & \Primal & \Literal & \Dual & \SPM \\
\midrule%
\csname @@input\endcsname experiments/final_flow/final_flow_running_time.tex 
\bottomrule
\multirow{2}{*}{Partitioner} & \multicolumn{7}{c}{Running Time $t[s]$} \\
\cmidrule{2-8}
 & \ALL & \DAC & \ISPD & \Primal & \Literal & \Dual & \SPM \\
\midrule%
\csname @@input\endcsname experiments/final_flow/final_flow_running_time_old.tex 
\bottomrule
\end{tabular} 
\caption{Comparing the average running time of \KaHyPar{MF} with \KaHyPar{CA} and
         other hypergraph partitioners.}
\label{tbl:running_time} 
\end{table}
